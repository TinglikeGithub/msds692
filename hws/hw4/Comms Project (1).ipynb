{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from words import get_text, words, filenames\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine Comparision: \n",
    "\n",
    "# Linear Search v.s. Hashtable Search\n",
    "\n",
    "Let us imagine we have a collection of about 4500 files, each being an article filled with numerous words. We aim to search for specific words across all files and return a list of filenames where the file content includes all the words in terms.\n",
    "\n",
    "We will try to carry out this objective with both a traditional linear search, as well as a method using hash tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filelist(root):\n",
    "    root = os.path.expanduser(root)  # Expand the user directory if present\n",
    "    retlist = []\n",
    "    \n",
    "    for root_dir, subdirs, files in os.walk(root):\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(root_dir, filename)\n",
    "            retlist.append(file_path)\n",
    "    \n",
    "    return retlist\n",
    "files = filelist(\"~/data/slate/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *files* variable is a list which contains around ~4500 articles from the online magazine Slate in a .txt format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4530"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Search\n",
    "\n",
    "terms = ['picture','perfect','for','and','because','since']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_search(files, terms):\n",
    "    retlist = []\n",
    "    corpus = [words(get_text(file)) for file in files]\n",
    "    for i in range(len(files)):\n",
    "        if set(terms) <= set(corpus[i]):\n",
    "            retlist.append(files[i])\n",
    "    return filenames(retlist)\n",
    "\n",
    "\n",
    "terms = ['picture','perfect','for','and','because','since']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Spent: 1.3 second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.19 s, sys: 113 ms, total: 1.3 s\n",
      "Wall time: 1.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Article247_3653.txt',\n",
       " 'Article247_4014.txt',\n",
       " 'ArticleIP_1268.txt',\n",
       " 'ArticleIP_1553.txt',\n",
       " 'ArticleIP_2725.txt',\n",
       " 'ArticleIP_2911.txt',\n",
       " 'ArticleIP_2922.txt',\n",
       " 'ArticleIP_2941.txt',\n",
       " 'ArticleIP_4062.txt',\n",
       " 'ArticleIP_20511.txt',\n",
       " 'ArticleIP_25134.txt',\n",
       " 'ArticleIP_25161.txt',\n",
       " 'ArticleIP_32882.txt',\n",
       " 'ArticleIP_38823.txt',\n",
       " 'ArticleIP_56271.txt']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time linear_search(files,terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-define the number of buckets in your hash table:\n",
    "- A limitation of hash tables is having to re-index everytime a bucket is added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def htable(nbuckets):\n",
    "    return [[]]*nbuckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create some sort of hash function to index a bucket\n",
    "- This should always return an integer\n",
    "- We can then use this integer to represent the location of the value associated with that bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashcode(o):\n",
    "    if isinstance(o, int):\n",
    "        return o\n",
    "    elif isinstance(o, str):\n",
    "        h = 0\n",
    "        for c in o:\n",
    "            h = h * 31 + ord(c)\n",
    "        return h\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and Writing to the Hash Table\n",
    "\n",
    "The htable put and htable get functions allow us to read and write values from our hash table. They work by running the key through a hash function to find the key's corresponding bucket. They then check the relevant bucket to see if any of the key, value pair tuples have the relevant key.\n",
    "\n",
    "If the relevant key is in the bucket, the htable_get function will return the relevant value. Meanwhile, the htable_put function will replace the value with the new value\n",
    "\n",
    "If the relevant key isn't in the bucket, the htable_get function will return None, meanwhile the htable_put function will append a new key value pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bucket_indexof(table, key):\n",
    "    return hashcode(key) % len(table)\n",
    "    \n",
    "def htable_put(table, key, value):\n",
    "    index = bucket_indexof(table, key)\n",
    "    bucket = table[index]\n",
    "\n",
    "    # Filter out existing (key, value) pairs with the same key\n",
    "    bucket = [(k, v) for k, v in bucket if k != key]\n",
    "\n",
    "    # Append the new (key, value) pair\n",
    "    bucket.append((key, value))\n",
    "\n",
    "    # Update the bucket in the table\n",
    "    table[index] = bucket\n",
    "\n",
    "    return table\n",
    "\n",
    "def htable_get(table, key):\n",
    "    index = bucket_indexof(table, key)\n",
    "    bucket = table[index]\n",
    "\n",
    "    for pair in bucket:\n",
    "        if key == pair[0]:\n",
    "            return pair[1]\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Hash Tables as Indexes\n",
    "Instead of the linear search, where you have to manually go through every file to check if the relevant search terms are present, using hashtables we can create an index. We create an index of each word to all the files they're in, with the key being the word and the value being a list of files the word is present in. \n",
    "\n",
    "Doing this, all we need to do to search for terms is to reference our index hash table and find the intersection of files for the relevant word indexes.\n",
    "\n",
    "The tradeoff between the linear search and using hash tables is in the initial creation of the index. After we create the index, searching becomes much faster than linear search. But the index takes a bit to create. So which one you want to use will come down to how many times you plan on using the search operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myhtable_create_index(files):\n",
    "    wordtab = htable(4011)\n",
    "    for doc in files:\n",
    "        wordlist = set(words(get_text(doc)))\n",
    "        for word in wordlist:\n",
    "            oldval = htable_get(wordtab,word)\n",
    "            if oldval:\n",
    "                newval = oldval\n",
    "            else:\n",
    "                newval = []\n",
    "            newval.append(doc)\n",
    "            wordtab = htable_put(wordtab,word,newval)\n",
    "    return wordtab\n",
    "\n",
    "\n",
    "def myhtable_index_search(files, index, terms):\n",
    "    sets_list = []\n",
    "    for w in terms:\n",
    "        info = htable_get(index, w)\n",
    "        if info:\n",
    "            sets_list.append(set(info))\n",
    "    \n",
    "    if not sets_list:\n",
    "        return sets_list\n",
    "\n",
    "    # Calculate the intersection of all sets in the 'sets_list' using set.intersection. The * unpacks the sets\n",
    "    allmatches = set.intersection(*sets_list)\n",
    "    \n",
    "    return allmatches.intersection(set(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Spent: 15 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.7 s, sys: 57.5 ms, total: 14.8 s\n",
      "Wall time: 14.8 s\n"
     ]
    }
   ],
   "source": [
    "%time index = myhtable_create_index(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Spent: 613 µs\n",
    "\n",
    "1 second = 1,000,000 microseconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 609 µs, sys: 0 ns, total: 609 µs\n",
      "Wall time: 613 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'/home/karthik/data/slate/13/Article247_3653.txt',\n",
       " '/home/karthik/data/slate/17/Article247_4014.txt',\n",
       " '/home/karthik/data/slate/24/ArticleIP_1268.txt',\n",
       " '/home/karthik/data/slate/27/ArticleIP_1553.txt',\n",
       " '/home/karthik/data/slate/39/ArticleIP_2725.txt',\n",
       " '/home/karthik/data/slate/40/ArticleIP_2911.txt',\n",
       " '/home/karthik/data/slate/40/ArticleIP_2922.txt',\n",
       " '/home/karthik/data/slate/40/ArticleIP_2941.txt',\n",
       " '/home/karthik/data/slate/48/ArticleIP_4062.txt',\n",
       " '/home/karthik/data/slate/50/ArticleIP_20511.txt',\n",
       " '/home/karthik/data/slate/50/ArticleIP_25134.txt',\n",
       " '/home/karthik/data/slate/50/ArticleIP_25161.txt',\n",
       " '/home/karthik/data/slate/51/ArticleIP_32882.txt',\n",
       " '/home/karthik/data/slate/51/ArticleIP_38823.txt',\n",
       " '/home/karthik/data/slate/53/ArticleIP_56271.txt'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time myhtable_index_search(files, index, terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
